<div>
<h2>&#30446;&#24405;</h2>
<div>
<ul>
<li><a href="http://www.zmonster.me#orgb88d858" rel="noopener noreferrer" target="_blank">&#20851;&#20110;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#org9f9025e" rel="noopener noreferrer" target="_blank">&#36393;&#22353;&#21015;&#34920;</a>
<ul>
<li><a href="http://www.zmonster.me#orgffa60ab" rel="noopener noreferrer" target="_blank">&#33150;&#35759;&#30340; NeuralClassifier</a></li>
<li><a href="http://www.zmonster.me#orga6e1f68" rel="noopener noreferrer" target="_blank">&#26080;&#20154;&#32500;&#25252;&#30340; keras-text</a></li>
<li><a href="http://www.zmonster.me#org0a650e3" rel="noopener noreferrer" target="_blank">&#24046;&#24378;&#20154;&#24847;&#30340; text-classification-keras</a></li>
</ul>
</li>
<li><a href="http://www.zmonster.me#org96810e9" rel="noopener noreferrer" target="_blank">&#21487;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;&#24037;&#20855;&#21450;&#20854;&#20351;&#29992;&#26041;&#27861;</a>
<ul>
<li><a href="http://www.zmonster.me#org103b445" rel="noopener noreferrer" target="_blank">&#20351;&#29992; NLTK &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#org6d13b9e" rel="noopener noreferrer" target="_blank">&#20351;&#29992; TextBlob &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#org2225d3a" rel="noopener noreferrer" target="_blank">&#20351;&#29992; TextGrocery &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#org98269f3" rel="noopener noreferrer" target="_blank">&#20351;&#29992; sklearn &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#org4428836" rel="noopener noreferrer" target="_blank">&#20351;&#29992; FastText &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#org243d4a6" rel="noopener noreferrer" target="_blank">&#20351;&#29992; Kashgari &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a>
<ul>
<li><a href="http://www.zmonster.me#orgc94711d" rel="noopener noreferrer" target="_blank">&#36827;&#34892;&#24120;&#35268;&#30340;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#org6d4406e" rel="noopener noreferrer" target="_blank">&#22522;&#20110; BERT &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a></li>
</ul>
</li>
<li><a href="http://www.zmonster.me#orgd485d3f" rel="noopener noreferrer" target="_blank">&#20351;&#29992; AllenNLP &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a>
<ul>
<li><a href="http://www.zmonster.me#org3cb8159" rel="noopener noreferrer" target="_blank">&#36827;&#34892;&#24120;&#35268;&#30340;&#25991;&#26412;&#20998;&#31867;</a></li>
<li><a href="http://www.zmonster.me#orgcdca18c" rel="noopener noreferrer" target="_blank">&#22522;&#20110; BERT &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<p>
&#26412;&#25991;&#26159;&#12298;NLP &#21738;&#37324;&#36305;&#12299;&#31995;&#21015;&#30340;&#31532;&#22235;&#31687;&#25991;&#31456;&#65292;&#31995;&#21015;&#25991;&#31456;&#22914;&#19979;&#65306;
</p>
<ol>
<li><a href="http://www.zmonster.me/2018/03/26/nlp-thinking-1.html" rel="noopener noreferrer" target="_blank">NLP&#21738;&#37324;&#36305;: &#24320;&#31687;&#21450;&#19968;&#20123;&#30862;&#30862;&#24565; &middot; ZMonster's Blog</a></li>
<li><a href="http://www.zmonster.me/2018/06/25/nlp-thinking-2.html" rel="noopener noreferrer" target="_blank">NLP&#21738;&#37324;&#36305;: &#20160;&#20040;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; &middot; ZMonster's Blog</a></li>
<li><a href="http://www.zmonster.me/2018/10/20/nlp-road-3-unicode.html" rel="noopener noreferrer" target="_blank">NLP&#21738;&#37324;&#36305;: Unicode&#30456;&#20851;&#30340;&#19968;&#20123;&#23567;&#30693;&#35782;&#21644;&#24037;&#20855; &middot; ZMonster's Blog</a></li>
<li><a href="http://www.zmonster.me/2019/10/20/nlp-thinking-4.html" rel="noopener noreferrer" target="_blank">NLP&#21738;&#37324;&#36305;: &#25991;&#26412;&#20998;&#31867;&#24037;&#20855;&#19968;&#35272; &middot; ZMonster's Blog</a></li>
</ol>

<div>
<h2>&#20851;&#20110;&#25991;&#26412;&#20998;&#31867;</h2>
<div>
<p>
&#25152;&#35859;&#30340;&#25991;&#26412;&#20998;&#31867;&#65292;&#20854;&#23454;&#23601;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#31867;&#38382;&#39064;&#22312; NLP &#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#23427;&#30340;&#29702;&#35770;&#30456;&#23545;&#31616;&#21333;&#12289;&#24037;&#20855;&#25104;&#29087;&#12289;&#19978;&#25163;&#31616;&#21333;&#65292;&#22823;&#23478;&#22522;&#26412;&#19978;&#26159;&#25226;&#23427;&#24403;&#20316;&#19968;&#20010;&#12300;&#29702;&#35770;&#19978;&#24050;&#32463;&#35299;&#20915;&#12301;&#30340;&#38382;&#39064;&#26469;&#30475;&#24453;&#65292;&#20294;&#20854;&#23454;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#22788;&#29702;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#26102;&#36824;&#26159;&#20250;&#36935;&#21040;&#19981;&#23569;&#38382;&#39064;&#30340;&#65292;&#21152;&#19978;&#25991;&#26412;&#20998;&#31867;&#21448;&#26159; NLP &#39046;&#22495;&#20013;&#26368;&#24120;&#35265;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#25105;&#24819;&#25226;&#33258;&#24049;&#22312;&#36825;&#26041;&#38754;&#30340;&#19968;&#20123;&#32463;&#39564;&#21644;&#23398;&#20064;&#25104;&#26524;&#24930;&#24930;&#22320;&#25972;&#29702;&#20986;&#26469;&#12290;
</p>

<p>
2017 &#24180;&#30340;&#26102;&#20505;&#65292;&#20026;&#20102;&#25552;&#39640;&#20570;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23558; <a href="https://scikit-learn.org/stable/index.html" rel="noopener noreferrer" target="_blank">sklearn</a> &#20013;&#30340;&#25991;&#26412;&#20998;&#31867;&#21151;&#33021;&#20570;&#20102;&#19968;&#20123;&#23553;&#35013;&#65292;&#21518;&#26469;&#26029;&#26029;&#32493;&#32493;&#22320;&#20248;&#21270;&#65292;&#20135;&#20986;&#20102;&#19968;&#20010;&#25105;&#33258;&#24049;&#29992;&#36215;&#26469;&#24456;&#39034;&#25163;&#30340;&#25991;&#26412;&#20998;&#31867;&#24037;&#20855;&#12290;&#22312;&#25105;&#24320;&#22987;&#20889;&#12298;NLP &#21738;&#37324;&#36305;&#12299;&#36825;&#20010;&#31995;&#21015;&#30340;&#21338;&#23458;&#21518;&#65292;&#25105;&#35745;&#21010;&#26159;&#25226;&#33258;&#24049;&#22312; NLP &#26041;&#38754;&#30340;&#32463;&#39564;&#36827;&#34892;&#31995;&#32479;&#22320;&#26803;&#29702;&#65292;&#31532;&#19968;&#22359;&#23601;&#26159;&#25991;&#26412;&#20998;&#31867; &mdash;&mdash; &#36825;&#19968;&#22359;&#24403;&#28982;&#26377;&#24456;&#22810;&#24456;&#22810;&#24819;&#35762;&#30340;&#19996;&#35199;&#21644;&#24819;&#20570;&#30340;&#20107;&#24773;&#65292;&#19968;&#31687;&#25991;&#31456;&#26159;&#20889;&#19981;&#23436;&#30340;&#65292;&#25152;&#20197;&#26368;&#21021;&#30340;&#24819;&#27861;&#26159;&#20808;&#30475;&#19968;&#19979;&#24037;&#20855;&#26041;&#38754;&#30340;&#24773;&#20917;&#12290;&#25105;&#23545;&#25105;&#22312;&#20844;&#21496;&#32500;&#25252;&#30340;&#25991;&#26412;&#20998;&#31867;&#24037;&#20855;&#36824;&#26159;&#25402;&#28385;&#24847;&#30340;&#65292;&#20294;&#20063;&#20250;&#24819;&#33258;&#24049;&#20250;&#19981;&#20250;&#23545;&#19968;&#20123;&#20808;&#36827;&#30340;&#24037;&#20855;&#35748;&#35782;&#19981;&#22815;&#65292;&#25152;&#20197;&#23601;&#21435;&#20102;&#35299;&#20102;&#24456;&#22810;&#20854;&#20182;&#30340;&#21516;&#31867;&#24037;&#20855;&#65292;&#26412;&#31687;&#25991;&#31456;&#23601;&#26159;&#23545;&#36825;&#20123;&#24037;&#20855;&#30340;&#19968;&#20010;&#31616;&#21333;&#30340;&#32599;&#21015;&#65292;&#19981;&#28041;&#21450;&#20998;&#31867;&#27169;&#22411;&#30340;&#29702;&#35770;&#65292;&#20063;&#19981;&#28041;&#21450;&#26576;&#20010;&#20998;&#31867;&#27169;&#22411;&#30340;&#20855;&#20307;&#23454;&#29616;&#30340;&#20248;&#21155;&#35780;&#20215;&#65292;&#20165;&#20165;&#26159;&#19968;&#20010;&#38750;&#24120;&#24037;&#20855;&#21521;&#12289;&#23454;&#29992;&#21521;&#30340;&#25972;&#29702;&#35760;&#24405;&#12290;
</p>

<p>
&#25105;&#22312;&#25361;&#36873;&#25991;&#26412;&#20998;&#31867;&#24037;&#20855;&#26102;&#26159;&#26377;&#19968;&#20123;&#26631;&#20934;&#30340;&#65292;&#19981;&#26159;&#38750;&#24120;&#20005;&#26684;&#65292;&#20294;&#22823;&#27010;&#33021;&#20998;&#25104;&#20197;&#19979;&#20960;&#28857;&#65306;
</p>
<ul>
<li>&#24037;&#31243;&#21270;&#31243;&#24230;&#33391;&#22909;&#30340;&#65292;&#33021;&#25552;&#20379;&#26131;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#25110;&#21629;&#20196;&#34892;&#25509;&#21475;</li>
<li>&#20197; Python &#29983;&#24577;&#20869;&#30340;&#24037;&#20855;&#20026;&#20027; &mdash;&mdash; &#24456;&#22810;&#20854;&#20182;&#35821;&#35328;&#23454;&#29616;&#30340;&#21516;&#31867;&#24037;&#20855;&#65292;&#38480;&#20110;&#31934;&#21147;&#23601;&#27809;&#26377;&#20102;&#35299;&#20102;</li>
</ul>

<p>
&#21518;&#38754;&#30340;&#20869;&#23481;&#20250;&#20998;&#25104;&#20004;&#22359;&#65306;&#31532;&#19968;&#37096;&#20998;&#35762;&#25105;&#30340;&#36393;&#22353;&#32463;&#21382;&#65292;&#20027;&#35201;&#26159;&#19968;&#20123;&#26412;&#26469;&#20197;&#20026;&#20250;&#22909;&#29992;&#30340;&#24037;&#20855;&#32467;&#26524;&#21457;&#29616;&#19981;&#31526;&#21512;&#25105;&#26631;&#20934;&#30340;&#24773;&#20917;&#65307;&#31532;&#20108;&#37096;&#20998;&#26159;&#25105;&#23454;&#39564;&#20043;&#21518;&#30830;&#35748;&#21487;&#29992;&#30340;&#24037;&#20855;&#21644;&#23427;&#20204;&#30340;&#20351;&#29992;&#26041;&#27861;&#12290;
</p>
</div>
</div>

<div>
<h2>&#36393;&#22353;&#21015;&#34920;</h2>
<div>
<p>
&#26412;&#33410;&#20013;&#21015;&#20030;&#30340;&#24037;&#20855;&#65292;&#24314;&#35758;&#35835;&#32773;&#19981;&#35201;&#28010;&#36153;&#26102;&#38388;&#22312;&#19978;&#38754;&#12290;
</p>
</div>

<div>
<h3>&#33150;&#35759;&#30340; NeuralClassifier</h3>
<div>
<p>
<b>&#27880;&#65306;&#19979;&#25991;&#20165;&#20195;&#34920;&#20010;&#20154;&#35266;&#28857;&#21644;&#24863;&#21463;&#65292;&#19981;&#26381;&#19981;&#31649;&#12290;</b>
</p>

<p>
&#19977;&#20010;&#26376;&#21069;&#33150;&#35759;&#24320;&#28304;&#30340;&#65292;&#35265;&#65306;<a href="https://cloud.tencent.com/developer/article/1459733" rel="noopener noreferrer" target="_blank">&#12304;&#24320;&#28304;&#20844;&#21578;&#12305;NeuralNLP-NeuralClassifier - &#28145;&#24230;&#23398;&#20064;&#25991;&#26412;&#20998;&#31867;&#24037;&#20855; - &#20113;+&#31038;&#21306; - &#33150;&#35759;&#20113;</a>&#12290;
</p>

<p>
&#39033;&#30446;&#22320;&#22336;&#65306;<a href="https://github.com/Tencent/NeuralNLP-NeuralClassifier" rel="noopener noreferrer" target="_blank">https://github.com/Tencent/NeuralNLP-NeuralClassifier</a>
</p>

<p>
&#22312;&#25105;&#30340;&#21015;&#34920;&#37324;&#26368;&#22353;&#30340;&#19968;&#20010;&#65306;
</p>
<ul>
<li>&#20316;&#20026;&#19968;&#20010; Python &#39033;&#30446;&#65292;&#27809;&#26377;&#21457;&#24067;&#21040; pypi &#23601;&#31639;&#20102;&#65292;&#36830; setup.py &#20063;&#27809;&#26377;&#65292;&#26080;&#27861;&#23433;&#35013;&#65292;&#39033;&#30446;&#32452;&#32455;&#20063;&#27809;&#30524;&#30475;&#65292;&#21482;&#33021;&#20687;&#20010;&#21407;&#22987;&#20154;&#19968;&#26679;&#25335;&#36125;&#20195;&#30721;&#21040;&#33258;&#24049;&#30340;&#30446;&#24405;&#36305;&#36305;&#33050;&#26412;&#65292;&#24456;&#38590;&#24819;&#35937;&#26159;&#19968;&#20010;&#22823;&#21378;&#30340;&#39033;&#30446;</li>
<li>&#35757;&#32451;&#38656;&#35201;&#20351;&#29992;&#19968;&#20010;&#26684;&#24335;&#22797;&#26434;&#30340; json &#26684;&#24335;&#30340;&#37197;&#32622;&#25991;&#20214;&#65292;&#28982;&#21518;&#36825;&#20010;&#37197;&#32622;&#30340;&#25991;&#26723;&#22826;&#36807;&#31616;&#30053;&#65292;&#19981;&#23569;&#32454;&#33410;&#34255;&#22312;&#39033;&#30446;&#25552;&#20379;&#30340;&#33050;&#26412;&#37324;&hellip;&hellip;</li>
</ul>

<p>
&#26368;&#32456;&#25105;&#39764;&#25913;&#20102;&#19968;&#36890;&#21518;&#36305;&#36215;&#26469;&#20102;&#65292;&#20294;&#26159;&#24050;&#32463;&#24694;&#24515;&#21040;&#25105;&#20102;&#65292;&#24323;&#12290;
</p>
</div>
</div>

<div>
<h3>&#26080;&#20154;&#32500;&#25252;&#30340; keras-text</h3>
<div>
<p>
&#39033;&#30446;&#22320;&#22336;&#65306;<a href="https://github.com/raghakot/keras-text" rel="noopener noreferrer" target="_blank">https://github.com/raghakot/keras-text</a>
</p>

<p>
&#36825;&#20010;&#39033;&#30446;&#25105;&#35273;&#24471;&#34542;&#21487;&#24796;&#30340;&#65292;&#20174;&#25991;&#26723;&#21644;&#23454;&#38469;&#20351;&#29992;&#26469;&#30475;&#65292;&#20316;&#32773;&#22312;&#20195;&#30721;&#32467;&#26500;&#21644;&#20351;&#29992;&#27969;&#31243;&#19978;&#26159;&#20570;&#20102;&#19968;&#20123;&#29992;&#24515;&#30340;&#35774;&#35745;&#30340;&#65292;&#20294;&#26159;&#26377;&#20123;&#20851;&#38190;&#27169;&#22359;&#27809;&#26377;&#23436;&#25104;&#65292;&#22312;&#24456;&#22810;&#32454;&#33410;&#19978;&#23384;&#22312;&#20196;&#20154;&#38590;&#20197;&#24525;&#21463;&#30340;&#23567;&#38382;&#39064;&#12290;
</p>

<p>
&#39033;&#30446;&#24050;&#32463;&#20004;&#24180;&#27809;&#26377;&#26356;&#26032;&#20102;&#65292;&#21487;&#20197;&#21442;&#32771;&#23427;&#30340;&#20195;&#30721;&#65292;&#20294;&#19981;&#24314;&#35758;&#20316;&#20026;&#19968;&#20010;&#27491;&#32463;&#30340;&#24037;&#20855;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;
</p>
</div>
</div>

<div>
<h3>&#24046;&#24378;&#20154;&#24847;&#30340; text-classification-keras</h3>
<div>
<p>
&#39033;&#30446;&#22320;&#22336;&#65306;<a href="https://github.com/jfilter/text-classification-keras" rel="noopener noreferrer" target="_blank">https://github.com/jfilter/text-classification-keras</a>
</p>

<p>
&#35813;&#39033;&#30446;&#26159;&#23545; keras-text &#39033;&#30446;&#30340;&#25913;&#36827;&#65292;&#24635;&#20307;&#19978;&#26469;&#35828;&#26159;&#19968;&#20010;&#21487;&#29992;&#30340;&#39033;&#30446;&#65292;&#20294;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;
</p>
<ul>
<li>&#20351;&#29992;&#25991;&#26723;&#19981;&#40784;&#20840;</li>
<li>&#20195;&#30721;&#19978;&#20173;&#28982;&#26377;&#19968;&#20123;&#33268;&#21629;&#20260;&#65292;&#27604;&#22914;

<ul>
<li>&#23558;&#25991;&#26412;&#36716;&#25104;&#29305;&#24449;&#21521;&#37327;&#21518;&#65292;&#20808;&#23384;&#20102;&#19968;&#20010;&#25991;&#20214;&#65292;&#28982;&#21518;&#20174;&#25991;&#20214;&#20013;&#21152;&#36733;&#21518;&#20877;&#21890;&#32473;&#27169;&#22411;&hellip;&hellip;&#24847;&#20041;&#19981;&#26126;&hellip;&hellip;</li>
<li>&#27599;&#27425;&#35843;&#29992;&#30340;&#26102;&#20505;&#37117;&#35201;&#25226; <a href="https://spacy.io" rel="noopener noreferrer" target="_blank">spaCy</a> &#30340;&#27169;&#22411;&#37325;&#26032;&#21152;&#36733;&#19968;&#36941;&#65292;&#24930;&#24471;&#35201;&#27515;</li>
</ul></li>
</ul>

<p>
&#20316;&#32773;&#24212;&#35813;&#26159;&#20174; keras-text &#39033;&#30446; fork &#36807;&#26469;&#28982;&#21518;&#25913;&#25104;&#33021;&#29992;&#30340;&#29366;&#24577;&#30340;&#65292;&#20063;&#25402;&#19981;&#23481;&#26131;&#30340;&#65292;&#20294;&#19981;&#31649;&#24590;&#20040;&#35828;&#22312;&#25105;&#36825;&#37324;&#26159;&#19968;&#20010;&#19981;&#21512;&#26684;&#30340;&#24037;&#20855;&#12290;
</p>
</div>
</div>
</div>

<div>
<h2>&#21487;&#29992;&#30340;&#25991;&#26412;&#20998;&#31867;&#24037;&#20855;&#21450;&#20854;&#20351;&#29992;&#26041;&#27861;</h2>
<div>
</div>
<div>
<h3>&#20351;&#29992; NLTK &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h3>
<div>
<p>
&#23433;&#35013;: <code>pip install nltk</code>
</p>

<p>
&#25991;&#26723;: <a href="https://www.nltk.org/api/nltk.classify.html" rel="noopener noreferrer" target="_blank">https://www.nltk.org/api/nltk.classify.html</a>
</p>

<p>
NLTK &#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#25991;&#26412;&#22788;&#29702;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#20998;&#31867;&#22120;&#25509;&#21475;&#65292;&#32452;&#21512;&#36215;&#26469;&#23601;&#33021;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20102;&#12290;
</p>

<p>
&#20197;&#20854;&#20013;&#30340;&#26420;&#32032;&#36125;&#21494;&#26031; NaiveBayesClassifier &#20026;&#20363;&#65292;&#21487;&#20197;&#36825;&#26679;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;
</p>
<ul>
<li><p>
&#23454;&#29616;&#19968;&#20010;&#26041;&#27861;&#65292;&#23558;&#25991;&#26412;&#36716;&#25104; dict &#24418;&#24335;&#30340;&#29305;&#24449;
</p>

<p>
&#20197;&#33521;&#25991;&#20026;&#20363;&#65292;&#21487;&#20197;&#30452;&#25509;&#29992; NLTK &#20013;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#38656;&#35201;&#30340;&#35805;&#36824;&#21487;&#20197;&#21152;&#19978; <a href="https://en.wikipedia.org/wiki/Stemming" rel="noopener noreferrer" target="_blank">stemming</a> &#25110;&#32773; <a href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener noreferrer" target="_blank">lemmatization</a>&#12290;
</p>

<div>
<pre><span>from</span> nltk.corpus <span>import</span> stopwords
<span>from</span> nltk.tokenize <span>import</span> word_tokenize


<span>def</span> <span>extract_feature</span>(text):
    <span>feature</span> = {}
    <span>for</span> word <span>in</span> word_tokenize(text):
        <span>if</span> word <span>not</span> <span>in</span> stopwords:
            <span>feature</span>[word] = 1

    <span>return</span> feature
</pre>
</div>

<p>
&#20013;&#25991;&#30340;&#35805;&#21487;&#20197;&#25442;&#25104; jieba &#25110;&#32773;&#20854;&#20182;&#20013;&#25991;&#20998;&#35789;&#24037;&#20855;&#12290;
</p></li>

<li><p>
&#35757;&#32451;
</p>

<div>
<pre><span>from</span> nltk.classify <span>import</span> NaiveBayesClassifier


<span>train_texts</span> = [
    <span># </span><span>...</span>
]
<span>train_labels</span> = [
    <span># </span><span>...</span>
]

<span>train_features</span> = [extract_feature(text) <span>for</span> text <span>in</span> train_texts]
<span>train_samples</span> = <span>list</span>(<span>zip</span>(train_features, train_labels))
<span>classifier</span> = NaiveBayesClassifier.train(train_samples)
</pre>
</div></li>

<li><p>
&#35780;&#20272;
</p>

<div>
<pre><span>from</span> nltk.classify <span>import</span> accuracy

<span>test_texts</span> = [
    <span># </span><span>...</span>
]
<span>test_labels</span> = [
    <span># </span><span>...</span>
]

<span>test_features</span> = [extract_feature(text) <span>for</span> text <span>in</span> test_texts]
<span>test_samples</span> = <span>list</span>(<span>zip</span>(test_features, test_labels))
<span>acc</span> = accuracy(classifier, test_samples)
</pre>
</div></li>

<li><p>
&#39044;&#27979;
</p>

<p>
&#29992; classify &#26041;&#27861;&#30452;&#25509;&#39044;&#27979;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;
</p>
<div>
<pre><span>text</span> = <span>"blablabla"</span>              <span># </span><span>&#24453;&#39044;&#27979;&#30340;&#25991;&#26412;</span>
<span>feature</span> = extract_feature(text)
<span>pred_label</span> = classifier.classify(feature)
</pre>
</div>

<p>
&#29992; prob_classify &#26041;&#27861;&#33719;&#24471;&#25152;&#26377;&#21487;&#33021;&#31867;&#21035;&#30340;&#39044;&#27979;&#20998;&#25968;
</p>
<div>
<pre><span>text</span> = <span>"blablabla"</span>              <span># </span><span>&#24453;&#39044;&#27979;&#30340;&#25991;&#26412;</span>
<span>feature</span> = extract_feature(text)
<span>prob</span> = classifier.prob_classify(feature)
</pre>
</div></li>

<li><p>
&#27169;&#22411;&#20445;&#23384;&#21644;&#35835;&#21462;
</p>

<p>
&#21487;&#20197;&#30452;&#25509;&#29992; pickle &#20445;&#23384;&#12289;&#35835;&#21462;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;
</p>

<p>
&#20445;&#23384;&#65306;
</p>
<div>
<pre><span>import</span> pickle

<span>with</span> <span>open</span>(<span>'model.pkl'</span>, <span>'wb'</span>) <span>as</span> f:
    pickle.dump(classifier, f)
</pre>
</div>

<p>
&#35835;&#21462;&#65306;
</p>
<div>
<pre><span>import</span> pickle

<span>classifier</span> = <span>None</span>
<span>with</span> <span>open</span>(<span>'model.pkl'</span>, <span>'rb'</span>) <span>as</span> f:
    <span>classifier</span> = pickle.load(f)
</pre>
</div></li>
</ul>

<p>
NLTK &#20013;&#36824;&#26377;&#20854;&#20182;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#26041;&#27861;&#21644; NaiveBayesClassifier &#22823;&#21516;&#23567;&#24322;&#12290;
</p>
</div>
</div>

<div>
<h3>&#20351;&#29992; TextBlob &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h3>
<div>
<p>
<b>&#27880;&#24847;&#65306;TextBlob &#20165;&#25903;&#25345;&#33521;&#25991;</b>
</p>

<p>
&#23433;&#35013;: <code>pip install textblob</code>
</p>

<p>
&#25991;&#26723;: <a href="https://textblob.readthedocs.io/en/dev/classifiers.html" rel="noopener noreferrer" target="_blank">https://textblob.readthedocs.io/en/dev/classifiers.html</a>
</p>

<p>
TextBlob &#26159;&#19968;&#20010;&#22522;&#20110; NLTK &#30340;&#25991;&#26412;&#22788;&#29702;&#24037;&#20855;&#65292;&#20854;&#20013;&#30340;&#25991;&#26412;&#20998;&#31867;&#21151;&#33021;&#20063;&#26159;&#24314;&#31435;&#22312; NLTK &#20013;&#20998;&#31867;&#22120;&#30340;&#22522;&#30784;&#19978;&#30340;&#12290;
</p>

<ul>
<li><p>
&#35757;&#32451;
</p>

<div>
<pre><span>from</span> textblob.classifiers <span>import</span> NaiveBayesClassifier

<span>train_texts</span> = [
    <span># </span><span>...</span>
]
<span>train_labels</span> = [
    <span># </span><span>...</span>
]
<span>train_samples</span> = <span>list</span>(<span>zip</span>(train_texts, train_labels))
<span>classifier</span> = NaiveBayesClassifier(train_samples)
</pre>
</div></li>

<li><p>
&#35780;&#20272;
</p>

<div>
<pre><span>test_texts</span> = [
    <span># </span><span>...</span>
]
<span>test_labels</span> = [
    <span># </span><span>...</span>
]
<span>test_samples</span> = <span>list</span>(<span>zip</span>(test_texts, test_labels))
<span>acc</span> = classifier.accuracy(test_samples)
</pre>
</div></li>

<li><p>
&#39044;&#27979;
</p>

<p>
&#21482;&#26377;&#19968;&#20010; classify &#25509;&#21475;&#39044;&#27979;&#24471;&#21040;&#26368;&#26377;&#21487;&#33021;&#30340;&#31867;&#21035;
</p>
<div>
<pre><span>label</span> = classifier.classify(<span>"this is a sentence to be classified"</span>)
</pre>
</div></li>

<li><p>
&#27169;&#22411;&#20445;&#23384;&#21644;&#35835;&#21462;
</p>

<p>
&#21516; NLTK&#12290;
</p></li>
</ul>

<p>
&#30456;&#27604; NLTK &#20013;&#21407;&#26469;&#30340;&#25991;&#26412;&#20998;&#31867;&#65292;TextBlob &#30340;&#23553;&#35013;&#38544;&#34255;&#20102;&#19968;&#20123;&#32454;&#33410;&#65292;&#31616;&#21270;&#20102;&#25509;&#21475;&#65292;&#29992;&#36215;&#26469;&#36824;&#26159;&#25402;&#26041;&#20415;&#30340;&#12290;&#19981;&#22909;&#30340;&#19968;&#28857;&#26159;&#65292;TextBlob &#37324;&#24378;&#21046;&#20381;&#36182;&#20102; NLTK &#37324;&#30340; word_tokenize&#65292;&#34429;&#28982;&#35828; word_tokenize &#21487;&#20197;&#36890;&#36807; language &#21442;&#25968;&#35774;&#32622;&#35821;&#35328;&#65292;&#20294;&#22312; TextBlob &#37324;&#27809;&#26377;&#25552;&#20379;&#20256;&#36882;&#36825;&#20010;&#21442;&#25968;&#30340;&#26426;&#20250;&#65292;&#36825;&#23601;&#23548;&#33268; TextBlob &#21482;&#33021;&#23545;&#33521;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;
</p>
</div>
</div>

<div>
<h3>&#20351;&#29992; TextGrocery &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h3>
<div>
<p>
<b>&#27880;&#24847;&#65306;TextGrocery &#20165;&#25903;&#25345; Python2</b>
</p>

<p>
&#23433;&#35013;: <code>pip install tgrocery</code>
</p>

<p>
&#25991;&#26723;: <a href="https://github.com/2shou/TextGrocery/blob/master/README_CN.md" rel="noopener noreferrer" target="_blank">https://github.com/2shou/TextGrocery/blob/master/README_CN.md</a>
</p>

<ul>
<li><p>
&#21021;&#22987;&#21270;
</p>

<p>
&#38656;&#35201;&#32473;&#20998;&#31867;&#22120;&#25351;&#23450;&#19968;&#20010;&#21517;&#23383;
</p>
<div>
<pre><span>from</span> tgrocery <span>import</span> Grocery

<span>classifier</span> = Grocery(<span>'test'</span>)
</pre>
</div>

<p>
&#40664;&#35748;&#20351;&#29992; jieba &#20316;&#20026;&#20998;&#35789;&#22120;&#65292;&#20294;&#20063;&#25903;&#25345;&#22312;&#21021;&#22987;&#21270;&#20998;&#31867;&#22120;&#30340;&#26102;&#20505;&#36890;&#36807; custom_tokenize &#21442;&#25968;&#26469;&#33258;&#23450;&#20041;&#20998;&#35789;&#22120;
</p>
<div>
<pre><span>classifier</span> = Grocery(<span>'test'</span>, custom_tokenize=<span>list</span>)
</pre>
</div>

<p>
&#35201;&#27714; custom_tokenize &#30340;&#21442;&#25968;&#20540;&#26159;&#19968;&#20010; python &#30340;&#20989;&#25968;&#12290;
</p></li>

<li><p>
&#35757;&#32451;
</p>

<p>
&#25903;&#25345;&#20256;&#20837; python &#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65306;
</p>
<div>
<pre><span>train_src</span> = [
    (<span>'education'</span>, <span>'&#21517;&#24072;&#25351;&#23548;&#25176;&#31119;&#35821;&#27861;&#25216;&#24039;&#65306;&#21517;&#35789;&#30340;&#22797;&#25968;&#24418;&#24335;'</span>),
    (<span>'education'</span>, <span>'&#20013;&#22269;&#39640;&#32771;&#25104;&#32489;&#28023;&#22806;&#35748;&#21487; &#26159;&ldquo;&#29436;&#26469;&#20102;&rdquo;&#21527;&#65311;'</span>),
    (<span>'sports'</span>, <span>'&#22270;&#25991;&#65306;&#27861;&#32593;&#23391;&#33778;&#23572;&#26031;&#33510;&#25112;&#36827;16&#24378; &#23391;&#33778;&#23572;&#26031;&#24594;&#21564;'</span>),
    (<span>'sports'</span>, <span>'&#22235;&#24029;&#20025;&#26865;&#20030;&#34892;&#20840;&#22269;&#38271;&#36317;&#30331;&#23665;&#25361;&#25112;&#36187; &#36817;&#19975;&#20154;&#21442;&#19982;'</span>)
]
classifier.train(train_src)
</pre>
</div>

<p>
&#20063;&#25903;&#25345;&#20174;&#25991;&#20214;&#20013;&#35835;&#21462;&#35757;&#32451;&#25968;&#25454;&#28982;&#21518;&#35757;&#32451;&#65292;&#35201;&#27714;&#25991;&#20214;&#20013;&#19968;&#34892;&#26159;&#19968;&#20010;&#25968;&#25454;&#65292;&#19988;&#34892;&#20013;&#26377;&#19968;&#20010;&#20998;&#38548;&#31526;&#25226;&#25991;&#26412;&#21644;&#25991;&#26412;&#30340;&#31867;&#21035;&#26631;&#31614;&#20998;&#38548;&#24320;&#65292;&#22914;&#29992;&#31446;&#32447;&#20998;&#38548;&#65306;
</p>
<pre>
education|&#21517;&#24072;&#25351;&#23548;&#25176;&#31119;&#35821;&#27861;&#25216;&#24039;&#65306;&#21517;&#35789;&#30340;&#22797;&#25968;&#24418;&#24335;
education|&#20013;&#22269;&#39640;&#32771;&#25104;&#32489;&#28023;&#22806;&#35748;&#21487; &#26159;&ldquo;&#29436;&#26469;&#20102;&rdquo;&#21527;&#65311;
sports|&#22270;&#25991;&#65306;&#27861;&#32593;&#23391;&#33778;&#23572;&#26031;&#33510;&#25112;&#36827;16&#24378; &#23391;&#33778;&#23572;&#26031;&#24594;&#21564;
sports|&#22235;&#24029;&#20025;&#26865;&#20030;&#34892;&#20840;&#22269;&#38271;&#36317;&#30331;&#23665;&#25361;&#25112;&#36187; &#36817;&#19975;&#20154;&#21442;&#19982;
</pre>

<p>
&#20551;&#35774;&#19978;&#38754;&#30340;&#20869;&#23481;&#23384;&#20648;&#22312; train.txt &#20013;&#65292;&#21017;&#23558; train.txt &#20316;&#20026; train &#30340;&#21442;&#25968;&#65292;&#21516;&#26102;&#35201;&#29992; delimiter &#21442;&#25968;&#25351;&#26126;&#20998;&#38548;&#31526;
</p>
<div>
<pre>classifier.train(<span>'train.txt'</span>, delimiter=<span>'|'</span>)
</pre>
</div></li>

<li><p>
&#35780;&#20272;
</p>

<div>
<pre><span>test_src</span> = [
    (<span>'education'</span>, <span>'&#31119;&#24314;&#26149;&#23395;&#20844;&#21153;&#21592;&#32771;&#35797;&#25253;&#21517;18&#26085;&#25130;&#27490; 2&#26376;6&#26085;&#32771;&#35797;'</span>),
    (<span>'sports'</span>, <span>'&#24847;&#30002;&#39318;&#36718;&#34917;&#36187;&#20132;&#25112;&#35760;&#24405;:&#31859;&#20848;&#23458;&#22330;8&#25112;&#19981;&#36133;&#22269;&#31859;10&#24180;&#36830;&#32988;'</span>),
]
<span>report</span> = classifier.test(test_src)
report.show_result()
</pre>
</div>

<p>
&#19978;&#36848;&#20195;&#30721;&#20250;&#36755;&#20986;&#22914;&#19979;&#20869;&#23481;&#65306;
</p>
<pre>
               accuracy       recall
education      50.00%         100.00%
sports         0.00%          0.00%
</pre>

<p>
&#20063;&#21487;&#20197;&#20174;&#25991;&#20214;&#20013;&#35835;&#21462;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#25991;&#20214;&#30340;&#35201;&#27714;&#21516;&#35757;&#32451;
</p>
<div>
<pre><span>report</span> = classifier.test(<span>'test.txt'</span>, delimiter=<span>'|'</span>)
report.show_result()
</pre>
</div></li>

<li><p>
&#39044;&#27979;
</p>

<p>
&#20351;&#29992; predict &#25509;&#21475;&#26469;&#36827;&#34892;&#39044;&#27979;
</p>
<div>
<pre><span>preds</span> = classifier.predict(<span>'&#24847;&#30002;&#39318;&#36718;&#34917;&#36187;&#20132;&#25112;&#35760;&#24405;:&#31859;&#20848;&#23458;&#22330;8&#25112;&#19981;&#36133;&#22269;&#31859;10&#24180;&#36830;&#32988;'</span>)
<span>print</span> preds.dec_values         <span># </span><span>=&gt; {'education': 0.00604235155848336, 'sports': -0.006042351558483356}</span>
<span>print</span> preds.predicted_y        <span># </span><span>=&gt; education</span>
<span>print</span> <span>str</span>(preds)               <span># </span><span>=&gt; education</span>
</pre>
</div></li>

<li><p>
&#27169;&#22411;&#20445;&#23384;&#21644;&#35835;&#21462;
</p>

<p>
&#29992; save &#26041;&#27861;&#26469;&#20445;&#23384;&#27169;&#22411;
</p>
<div>
<pre>classifier.save()
</pre>
</div>

<p>
&#20445;&#23384;&#27169;&#22411;&#26102;&#20250;&#29992;&#20998;&#31867;&#22120;&#21021;&#22987;&#21270;&#26102;&#32473;&#30340;&#21517;&#23383;&#26469;&#21019;&#24314;&#19968;&#20010;&#30446;&#24405;&#65292;&#27604;&#22914;&#26368;&#24320;&#22987;&#32473;&#30340;&#21517;&#23383;&#26159; test&#65292;&#25152;&#20445;&#23384;&#30340;&#27169;&#22411;&#20250;&#22312; test &#30446;&#24405;&#19979;&#65292;&#22914;&#19979;&#25152;&#31034;&#65306;
</p>
<pre>
test
&#9500;&#9472;&#9472; converter
&#9474;&nbsp;&nbsp; &#9500;&#9472;&#9472; class_map.config.pickle
&#9474;&nbsp;&nbsp; &#9500;&#9472;&#9472; feat_gen.config.pickle
&#9474;&nbsp;&nbsp; &#9492;&#9472;&#9472; text_prep.config.pickle
&#9500;&#9472;&#9472; id
&#9492;&#9472;&#9472; learner
    &#9500;&#9472;&#9472; idf.pickle
    &#9500;&#9472;&#9472; liblinear_model
    &#9492;&#9472;&#9472; options.pickle
</pre>

<p>
&#29992;&#30456;&#21516;&#30340;&#21517;&#23383;&#21019;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#25191;&#34892; load &#26041;&#27861;&#26469;&#35835;&#21462;&#27169;&#22411;
</p>
<div>
<pre><span>classifier</span> = Grocery(<span>'test'</span>, custom_tokenize=<span>list</span>)
classifier.load()
</pre>
</div>

<p>
&#36825;&#37324;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#20445;&#23384;&#27169;&#22411;&#30340;&#26102;&#20505;&#65292;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#26159;&#27809;&#26377;&#34987;&#20445;&#23384;&#19979;&#26469;&#30340;&#65292;&#25152;&#20197;&#22312;&#35835;&#21462;&#30340;&#26102;&#20505;&#65292;&#36824;&#38656;&#35201;&#37325;&#26032;&#35774;&#32622;&#19968;&#19979;&#20998;&#35789;&#22120;&#12290;
</p></li>
</ul>

<p>
TextGrocery &#26159;&#19968;&#20010;&#22522;&#20110; liblinear &#30340;&#23567;&#24039;&#30340;&#25991;&#26412;&#20998;&#31867;&#23454;&#29616;&#65292;&#21487;&#24796;&#20316;&#32773;&#24050;&#32463;&#25918;&#24323;&#32500;&#25252;&#20102;&#65292;&#30446;&#21069;&#21482;&#33021;&#22312; Python2 &#29615;&#22659;&#37324;&#38754;&#20351;&#29992;&#12290;
</p>
</div>
</div>

<div>
<h3>&#20351;&#29992; sklearn &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h3>
<div>
<p>
&#23433;&#35013;: <code>pip install scikit-learn</code>
</p>

<p>
&#25991;&#26723;: <a href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#training-a-classifier" rel="noopener noreferrer" target="_blank">https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#training-a-classifier</a>
</p>

<p>
sklearn &#20013;&#23454;&#29616;&#20102;&#24456;&#22810;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#25105;&#20010;&#20154;&#26159;&#27604;&#36739;&#21916;&#27426;&#30340;&#12290;
</p>
<ul>
<li><p>
&#35757;&#32451;
</p>

<p>
&#39318;&#20808;&#21019;&#24314;&#19968;&#20010; vectorizer &#29992;&#26469;&#23558;&#25991;&#26412;&#32534;&#30721;&#25104;&#21521;&#37327;&#65292;&#26368;&#24120;&#29992;&#30340;&#21487;&#33021;&#26159; TfidfVectorizer &#20102;
</p>
<div>
<pre><span>from</span> sklearn.feature_extraction.text <span>import</span> TfidfVectorizer

<span>vectorizer</span> = TfidfVectorizer()
</pre>
</div>

<p>
&#40664;&#35748;&#20250;&#25353;&#31354;&#26684;&#26469;&#20998;&#35789;&#65292;&#22914;&#26524;&#38656;&#35201;&#33258;&#23450;&#20041;&#20998;&#35789;&#22120;&#65292;&#21487;&#20197;&#36890;&#36807; tokenizer &#21442;&#25968;&#26469;&#20256;&#20837;&#19968;&#20010;&#20989;&#25968;&#65292;&#27604;&#22914;
</p>
<div>
<pre><span>import</span> jieba

<span>def</span> <span>jieba_tokenize</span>(text):
    <span>return</span> jieba.lcut(text)

<span>vectorizer</span> = TfidfVectorizer(tokenizer=jieba_tokenize)
</pre>
</div>

<p>
<b>&#27880;&#24847;&#65306;&#30001;&#20110; jieba &#20013;&#21152;&#20102;&#32447;&#31243;&#38145;&#65292;&#23558; jieba.lcut &#30452;&#25509;&#20256;&#20837;&#65292;&#20250;&#23548;&#33268;&#27169;&#22411;&#26080;&#27861;&#20445;&#23384;</b>
</p>

<p>
&#36825;&#20010; vectorizer &#26159;&#38656;&#35201;&#35757;&#32451;&#30340;
</p>
<div>
<pre><span>texts</span> = [
    <span>'&#21517;&#24072;&#25351;&#23548;&#25176;&#31119;&#35821;&#27861;&#25216;&#24039;'</span>,
    <span>'&#20013;&#22269;&#39640;&#32771;&#25104;&#32489;&#28023;&#22806;&#35748;&#21487;'</span>,
    <span>'&#27861;&#32593;&#23391;&#33778;&#23572;&#26031;&#33510;&#25112;&#36827;16&#24378;'</span>,
    <span>'&#22235;&#24029;&#20025;&#26865;&#20030;&#34892;&#30331;&#23665;&#25361;&#25112;&#36187;'</span>,
]
vectorizer.fit(texts)
</pre>
</div>

<p>
&#19968;&#26086;&#35757;&#32451;&#21518;&#65292;&#23545;&#20219;&#24847;&#19968;&#20010;&#25991;&#26412;&#65292;&#20250;&#20135;&#29983;&#19968;&#20010;&#22266;&#23450;&#38271;&#24230;&#30340;&#21521;&#37327;&#65292;&#27604;&#22914;&#65306;
</p>
<div>
<pre><span>print</span>(vectorizer.transform([<span>'&#21517;&#24072;&#25351;&#23548;&#25176;&#31119;&#35821;&#27861;&#25216;&#24039;'</span>]).toarray())
</pre>
</div>

<p>
&#19978;&#38754;&#30340;&#20195;&#30721;&#20250;&#36755;&#20986;
</p>
<div>
<pre>[[0.        0.        0.        0.        0.4472136 0.        0.
  0.        0.        0.4472136 0.4472136 0.4472136 0.        0.
  0.        0.        0.        0.        0.        0.4472136 0.
  0.       ]]
</pre>
</div>

<p>
&#21521;&#37327;&#21270;&#36824;&#26377;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;&#19979;&#65306;
</p>
<div>
<pre><span>from</span> sklearn.feature_extraction.text <span>import</span> (
    TfidfVectorizer,
    CountVectorizer,
    HashingVectorizer,
)
<span>from</span> sklearn.feature_extraction <span>import</span> DictVectorizer
</pre>
</div>

<p>
&#21019;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#27604;&#22914; SVM
</p>
<div>
<pre><span>from</span> sklearn.svm <span>import</span> LinearSVC

<span>classifier</span> = LinearSVC()
</pre>
</div>

<p>
&#22914;&#26524;&#24819;&#20351;&#29992; GBDT &#20998;&#31867;&#22120;&#30340;&#35805;&#65292;&#21487;&#20197;&#25191;&#34892; <code>pip install xgboost</code> &#23433;&#35013; <a href="https://xgboost.readthedocs.io/en/latest/" rel="noopener noreferrer" target="_blank">XGBoost</a> &#36825;&#20010;&#21253;&#65292;&#23427;&#25552;&#20379;&#20102;&#31526;&#21512; sklearn &#35268;&#33539;&#30340;&#25509;&#21475;&#65292;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#24182;&#20687; sklearn &#30340;&#20998;&#31867;&#22120;&#19968;&#26679;&#29992;&#22312;&#21518;&#38754;&#30340;&#35757;&#32451;&#12289;&#39044;&#27979;&#36807;&#31243;&#20013;&#65306;
</p>
<div>
<pre><span>from</span> xgboost <span>import</span> XGBClassifier

<span>classifier</span> = XGBClassifier()
</pre>
</div>

<p>
&#39318;&#20808;&#29992; vectorizer &#23558;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#25991;&#26412;&#36716;&#25104;&#30697;&#38453;&#65292;&#28982;&#21518;&#21890;&#32473;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;
</p>
<div>
<pre><span>train_texts</span> = [
    <span># </span><span>blablabla</span>
]
<span>train_labels</span> = [
    <span># </span><span>blablabla</span>
]
<span>train_feats</span> = vectorizer.transform(train_texts)
classifier.fit(train_feats, train_labels)
</pre>
</div></li>

<li><p>
&#35780;&#20272;
</p>

<p>
&#29992;&#20998;&#31867;&#22120;&#30340; score &#26041;&#27861;&#21487;&#20197;&#35745;&#31639;&#27979;&#35797;&#38598;&#30340; accuracy
</p>
<div>
<pre><span>test_texts</span> = [
    <span># </span><span>...</span>
]
<span>test_labels</span> = [
    <span># </span><span>...</span>
]
<span>test_feats</span> = vectorizer.transform(test_texts)
<span>acc</span> = classifier.score(test_feats, test_labels)
</pre>
</div>

<p>
&#36825;&#20010;&#26041;&#27861;&#20854;&#23454;&#26159;&#35843;&#29992;&#20102; <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score" rel="noopener noreferrer" target="_blank">accuracy_score</a> &#36825;&#20010;&#20989;&#25968;&#65292;&#25152;&#20197;&#20063;&#21487;&#20197;&#33258;&#24049;&#26469;&#35745;&#31639;
</p>
<div>
<pre><span>from</span> sklearn.metrics <span>import</span> accuracy_score

<span>pred_labels</span> = classifier.predict(test_feats)
<span>acc</span> = accuracy_score(test_labels, pred_labels)
</pre>
</div>

<p>
&#36824;&#21487;&#20197;&#29992; <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html" rel="noopener noreferrer" target="_blank">classification_report</a> &#36825;&#20010;&#20989;&#25968;&#26469;&#24471;&#21040;&#26356;&#35814;&#32454;&#30340;&#35780;&#20272;&#25253;&#21578;
</p>
<div>
<pre><span>from</span> sklearn.metrics <span>import</span> classification_report

<span>pred_labels</span> = classifier.predict(test_feats)
<span>print</span>(classification_report(test_labels, pred_labels))
</pre>
</div>

<p>
&#36755;&#20986;&#32467;&#26524;&#26159;&#19979;&#38754;&#36825;&#20010;&#26679;&#23376;&#30340;&#65306;
</p>
<pre>
              precision    recall  f1-score   support

     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3

    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5
</pre>

<p>
&#26377;&#26102;&#20505;&#25105;&#20204;&#36824;&#38656;&#35201;&#36755;&#20986;&#20998;&#31867;&#30340;&#28151;&#28102;&#30697;&#38453;&#65292;&#34429;&#28982; sklearn &#25552;&#20379;&#20102; <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" rel="noopener noreferrer" target="_blank">sklearn.metrics.confusion_matrix</a> &#36825;&#20010;&#26041;&#27861;&#26469;&#35745;&#31639;&#28151;&#28102;&#30697;&#38453;&#65292;&#20294;&#23427;&#30340;&#36755;&#20986;&#19981;&#22815;&#30452;&#35266;&#65292;&#25105;&#20010;&#20154;&#27604;&#36739;&#21916;&#27426;&#29992; <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html" rel="noopener noreferrer" target="_blank">pandas.crosstab</a>
</p>
<div>
<pre><span>import</span> pandas

<span>pred_labels</span> = classifier.predict(test_feats)
<span>cnf_matrix</span> = pandas.crosstab(
    pandas.Series(test_labels), pandas.Series(pred_labels),
    rownames=[<span>'targets'</span>], colnames=[<span>'preds'</span>]
)
</pre>
</div>

<p>
&#36755;&#20837;&#32467;&#26524;&#26159;&#19979;&#38754;&#36825;&#20010;&#26679;&#23376;&#65306;
</p>
<pre>
preds     negative  positive

targets
negative       590       126
positive       383       901
</pre></li>

<li><p>
&#39044;&#27979;
</p>

<p>
&#29992; predict &#26041;&#27861;&#26469;&#39044;&#27979;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;&#65292;&#25110;&#29992; predict_proba &#26041;&#27861;&#26469;&#33719;&#24471;&#25152;&#39044;&#27979;&#31867;&#21035;&#30340;&#20998;&#25968;
</p>
<div>
<pre><span>texts</span> = [<span>'text1'</span>, <span>'text2'</span>, <span>'text3'</span>]
<span>feats</span> = vectorizer.transform(texts)
<span>labels</span> = classifier.predict(feats) <span># </span><span>labels: ['label1', 'label2', 'label3']</span>
<span># </span><span>or</span>
<span>prob_list</span> = classifier.predict_proba(feats)
<span># </span><span>prob_list:</span>
<span># </span><span>[</span>
<span>#     </span><span>{'label1': 0.1, 'label2': 0.3, 'label3': 0.6},</span>
<span>#     </span><span>{'label1': 0.1, 'label2': 0.3, 'label3': 0.6},</span>
<span>#     </span><span>{'label1': 0.1, 'label2': 0.3, 'label3': 0.6},</span>
<span># </span><span>]</span>
</pre>
</div>

<p>
&#27880;&#24847; sklearn &#20013;&#30340; predict/predict_proba &#37117;&#34987;&#35774;&#35745;&#20026;&#25209;&#37327;&#39044;&#27979;&#65292;&#27809;&#26377;&#21333;&#20010;&#25968;&#25454;&#39044;&#27979;&#30340;&#25509;&#21475;&#12290;
</p></li>

<li><p>
&#27169;&#22411;&#20445;&#23384;&#21644;&#35835;&#21462;
</p>

<p>
&#20445;&#23384;&#27169;&#22411;&#29992; pickle &#25110;&#32773; joblib &#37117;&#21487;&#20197;&#65292;&#27880;&#24847;&#35201;&#25226; vectorizer &#21644; classifier &#19968;&#36215;&#20445;&#23384;&#12290;
</p>
<div>
<pre><span>import</span> pickle
<span>from</span> sklearn.externals <span>import</span> joblib

<span>with</span> <span>open</span>(<span>'model.pkl'</span>, <span>'wb'</span>) <span>as</span> f:
    <span>data</span> = [vectorizer, classifier]
    pickle.dump(data, f)

<span># </span><span>or</span>
<span>data</span> = [vectorizer, classifier]
joblib.dump(data, <span>'model.pkl'</span>)
</pre>
</div>

<p>
&#22914;&#26524;&#20351;&#29992; <code>pickle.dump</code> &#20445;&#23384;&#30340;&#27169;&#22411;&#65292;&#21017;&#29992; <code>pickle.load</code> &#26469;&#35835;&#21462;&#65307;&#22914;&#26524;&#26159;&#29992; <code>joblib.dump</code> &#20445;&#23384;&#30340;&#21017;&#29992; <code>joblib.load</code> &#35835;&#21462;
</p>
<div>
<pre><span>vectorizer</span>, <span>classifier</span> = <span>None</span>, <span>None</span>
<span>with</span> <span>open</span>(<span>'model.pkl'</span>, <span>'rb'</span>) <span>as</span> f:
    <span>vectorizer</span>, <span>classifier</span> = pickle.load(f)

<span># </span><span>or</span>
<span>vectorizer</span>, <span>classifier</span> = joblib.load(<span>'model.pkl'</span>)
</pre>
</div></li>
</ul>


<p>
&#38500;&#20102;&#19978;&#38754;&#36825;&#26679;&#20808;&#21019;&#24314; vectorizer &#20877;&#21019;&#24314; classifier &#30340;&#26041;&#27861;&#65292;sklearn &#36824;&#25552;&#20379;&#20102; <a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html" rel="noopener noreferrer" target="_blank">Pipeline</a> &#36825;&#20010;&#31867;&#26469;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#38750;&#24120;&#25512;&#33616;&#20351;&#29992;&#12290;
</p>

<p>
&#21019;&#24314; vectorizer &#21644; classifier &#21518;&#65292;&#29992; Pipeline &#25226;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#65306;
</p>
<div>
<pre><span>from</span> sklearn.pipeline <span>import</span> Pipeline

<span>vectorizer</span> = TfidfVectorizer()
<span>classifier</span> = LinearSVC()
<span>pipeline</span> = Pipeline([(<span>'vec'</span>, vectorizer), (<span>'model'</span>, classifier)])
</pre>
</div>

<p>
&#28982;&#21518;&#21487;&#20197;&#30452;&#25509;&#23558;&#25991;&#26412;&#21890;&#32473; pipeline&#65292;&#19981;&#29992;&#33258;&#24049;&#20877;&#21435;&#35843;&#29992; vectorizer.fit &#21644; vectorizer.transform &#26469;&#23558;&#25991;&#26412;&#32534;&#30721;&#25104;&#21521;&#37327;&#20102;&#65281;
</p>
<div>
<pre><span>train_texts</span> = [
    <span># </span><span>blablabla</span>
]
<span>train_labels</span> = [
    <span># </span><span>blablabla</span>
]
pipeline.fit(train_texts, train_labels)
</pre>
</div>

<p>
&#35780;&#20272;&#12289;&#39044;&#27979;&#21644;&#38750; pipeline &#26041;&#24335;&#30340;&#24046;&#19981;&#22810;&#65292;&#37117;&#26159;&#21487;&#20197;&#30465;&#30053;&#25481;&#23558;&#25991;&#26412;&#36716;&#25104;&#21521;&#37327;&#30340;&#36825;&#20010;&#27493;&#39588;&#65307;&#27169;&#22411;&#20445;&#23384;&#26102;&#21482;&#38656;&#35201;&#23558; pipeline &#20445;&#23384;&#25104;&#25991;&#20214;&#21363;&#21487;&#12290;
</p>
</div>
</div>

<div>
<h3>&#20351;&#29992; FastText &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h3>
<div>
<p>
&#23433;&#35013;: <code>pip install fasttext</code>
</p>

<p>
&#25991;&#26723;: <a href="https://fasttext.cc/docs/en/python-module.html#text-classification-model" rel="noopener noreferrer" target="_blank">https://fasttext.cc/docs/en/python-module.html#text-classification-model</a>
</p>
<ul>
<li><p>
&#25968;&#25454;&#26684;&#24335;
</p>

<p>
fasttext &#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#37117;&#21482;&#33021;&#20174;&#25991;&#20214;&#20013;&#35835;&#21462;&#25968;&#25454;&#65292;&#32780;&#19981;&#33021;&#30452;&#25509;&#20256;&#20837; Python &#30340;&#20540;&#65292;&#32780;&#19988;&#23545;&#25991;&#20214;&#30340;&#26684;&#24335;&#26159;&#26377;&#35201;&#27714;&#30340;
</p>
<ol>
<li>&#25991;&#20214;&#20013;&#19968;&#34892;&#19968;&#20010;&#26679;&#26412;</li>
<li>&#27599;&#34892;&#29992;&#21046;&#34920;&#31526;&#20998;&#38548;&#65292;&#31532;&#19968;&#21015;&#26159;&#26631;&#31614;&#65292;&#31532;&#20108;&#21015;&#26159;&#25991;&#26412;</li>
<li>&#31532;&#19968;&#21015;&#30340;&#26631;&#31614;&#35201;&#26377; <code>__label__</code> &#21069;&#32512;</li>
<li>&#31532;&#20108;&#21015;&#30340;&#25991;&#26412;&#24517;&#39035;&#26159;&#29992;&#31354;&#26684;&#20998;&#38548;&#30340;&#35789;&#24207;&#21015;&#65292;&#23545;&#20013;&#25991;&#26469;&#35828;&#65292;&#24847;&#21619;&#30528;&#38656;&#35201;&#20808;&#20998;&#22909;&#35789;</li>
</ol>

<p>
&#25991;&#20214;&#20869;&#23481;&#31034;&#20363;&#22914;&#19979;&#65306;
</p>
<pre>
__label__education	&#21517;&#24072; &#25351;&#23548; &#25176;&#31119; &#35821;&#27861; &#25216;&#24039; &#65306; &#21517;&#35789; &#30340; &#22797;&#25968; &#24418;&#24335;
__label__education	&#20013;&#22269; &#39640;&#32771; &#25104;&#32489; &#28023;&#22806; &#35748;&#21487; &#26159; &ldquo; &#29436; &#26469;&#20102; &rdquo; &#21527; &#65311;
__label__sports	&#22270;&#25991; &#65306; &#27861;&#32593; &#23391;&#33778;&#23572;&#26031; &#33510;&#25112; &#36827; 16&#24378; &#23391;&#33778;&#23572;&#26031; &#24594;&#21564;
__label__sports	&#22235;&#24029; &#20025;&#26865; &#20030;&#34892; &#20840;&#22269; &#38271;&#36317; &#30331;&#23665; &#25361;&#25112;&#36187; &#36817; &#19975;&#20154; &#21442;&#19982;
</pre></li>

<li><p>
&#35757;&#32451;
</p>

<p>
&#20551;&#35774;&#35757;&#32451;&#25968;&#25454;&#25353;&#29031;&#21069;&#38754;&#30340;&#35201;&#27714;&#20889;&#22312;&#20102; train_data.txt &#37324;&#65292;&#21017;&#29992;&#19979;&#38754;&#30340;&#20195;&#30721;&#26469;&#35757;&#32451;&#65306;
</p>
<div>
<pre><span>import</span> fasttext

<span>model</span> = fasttext.train_supervised(<span>'train_data.txt'</span>)
</pre>
</div></li>

<li><p>
&#35780;&#20272;
</p>

<p>
&#20551;&#35774;&#27979;&#35797;&#25968;&#25454;&#22312; test_data.txt &#20013;&#65292;&#20351;&#29992; test &#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#25928;&#26524;&#65292;&#23427;&#20250;&#36820;&#22238;&#25968;&#25454;&#38598;&#20013;&#30340;&#26679;&#26412;&#25968;&#37327;&#65292;&#20197;&#21450; precesion &#21644; recall &#20540;&#65306;
</p>
<div>
<pre><span>num</span>, <span>precesion</span>, <span>recall</span> = model.test(<span>'test_data.txt'</span>)
</pre>
</div>

<p>
&#20063;&#21487;&#20197;&#29992; test_label &#26041;&#27861;&#33719;&#24471;&#27599;&#20010;&#31867;&#21035;&#30340; precesion&#12289;recall &#21644; f1 &#20540;&#65306;
</p>
<div>
<pre><span>print</span>(model.test_label(<span>'test_data.txt'</span>))
</pre>
</div>

<p>
&#36755;&#20986;&#32467;&#26524;&#26159;&#19979;&#38754;&#36825;&#20010;&#26679;&#23376;&#30340;&#65306;
</p>
<div>
<pre>{
    <span>'__label__education'</span>: {
        <span>'precision'</span>: 0.8830022075055187,
        <span>'recall'</span>: 0.8784773060029283,
        <span>'f1score'</span>: 0.8807339449541285
    },
    <span>'__label__sports'</span>: {
        <span>'precision'</span>: 0.883881230116649,
        <span>'recall'</span>: 0.853121801432958,
        <span>'f1score'</span>: 0.8682291666666667
    }
}
</pre>
</div></li>

<li><p>
&#39044;&#27979;
</p>

<p>
&#29992; predict &#25509;&#21475;&#26469;&#23545;&#21333;&#26465;&#25991;&#26412;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26679;&#35201;&#27714;&#25991;&#26412;&#26159;&#29992;&#31354;&#26684;&#20998;&#38548;&#30340;&#12289;&#20998;&#22909;&#35789;&#30340;
</p>
<div>
<pre><span>top3_labels</span>, <span>top3_scores</span> = model.predict(<span>'&#22303;&#35910;&#32593; &#25311; &#26126;&#24180; &#30331;&#38470; &#32435;&#24066; &#21215;&#36164; 1.5 &#20159;&#32654;&#20803;'</span>, k=3)
</pre>
</div></li>

<li><p>
&#27169;&#22411;&#20445;&#23384;&#21644;&#35835;&#21462;
</p>

<p>
&#20445;&#23384;
</p>
<div>
<pre>model.save_model(<span>'model.bin'</span>)
</pre>
</div>

<p>
&#35835;&#21462;
</p>
<div>
<pre><span>import</span> fasttext
<span>model</span> = fasttext.load_model(<span>'model.bin'</span>)
</pre>
</div></li>
</ul>
</div>
</div>


<div>
<h3>&#20351;&#29992; Kashgari &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h3>
<div>
<p>
&#23433;&#35013;: <code>pip install kashgari-tf tensorflow==1.14.0</code>
</p>

<p>
&#25991;&#26723;: <a href="https://kashgari.bmio.net/" rel="noopener noreferrer" target="_blank">https://kashgari.bmio.net/</a>
</p>

<p>
Kashgari &#26159;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340; NLP &#24037;&#20855;&#65292;&#20869;&#37096;&#23454;&#29616;&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20063;&#25903;&#25345;&#20102;&#26368;&#26032;&#30340; BERT&#65292;&#20351;&#29992;&#20307;&#39564;&#25402;&#19981;&#38169;&#30340;&#12290;
</p>
</div>

<div>
<h4>&#36827;&#34892;&#24120;&#35268;&#30340;&#25991;&#26412;&#20998;&#31867;</h4>
<div>
<ul>
<li><p>
&#35757;&#32451;
</p>

<p>
Kashgari &#35201;&#27714;&#36755;&#20837;&#30340;&#25991;&#26412;&#26159;&#20998;&#22909;&#35789;&#30340;&#65292;&#25226;&#20998;&#35789;&#30340;&#20107;&#24773;&#30041;&#32473;&#29992;&#25143;&#33258;&#24049;&#22788;&#29702;&#12290;&#19981;&#36807;&#20998;&#22909;&#35789;&#23601;&#33021;&#30452;&#25509;&#36755;&#20837;&#21040;&#27169;&#22411;&#20013;&#20102;&#65292;&#19981;&#38656;&#35201;&#20687; sklearn &#19968;&#26679;&#36890;&#36807; vectorizer &#36716;&#25104;&#21521;&#37327;&#65306;
</p>
<div>
<pre><span>from</span> kashgari.tasks.classification.models <span>import</span> CNN_Model

<span>train_x</span> = [[<span>'Hello'</span>, <span>'world'</span>], [<span>'Hello'</span>, <span>'Kashgari'</span>]]
<span>train_y</span> = [<span>'a'</span>, <span>'b'</span>]

<span>model</span> = CNN_Model()
model.fit(train_x, train_y)
</pre>
</div>

<p>
&#35757;&#32451;&#26102;&#36824;&#21487;&#20197;&#35774;&#32622;&#26657;&#39564;&#38598;
</p>
<div>
<pre><span>val_x</span> = [[<span>'Hello'</span>, <span>'world'</span>], [<span>'Hello'</span>, <span>'Kashgari'</span>]]
<span>val_y</span> = [<span>'a'</span>, <span>'b'</span>]

model.fit(val_x, val_y)
</pre>
</div></li>

<li><p>
&#35780;&#20272;
</p>

<div>
<pre><span>test_x</span> = [[<span>'Hello'</span>, <span>'world'</span>], [<span>'Hello'</span>, <span>'Kashgari'</span>]]
<span>test_y</span> = [<span>'a'</span>, <span>'b'</span>]
model.evaluate(test_x, test_y)
</pre>
</div>

<p>
&#20250;&#25171;&#21360;&#27979;&#35797;&#32467;&#26524;&#21040;&#26631;&#20934;&#36755;&#20986;&#65292;&#20854;&#20869;&#23481;&#26159;&#19979;&#38754;&#36825;&#20010;&#26684;&#24335;&#30340;&#65306;
</p>
<pre>
              precision    recall  f1-score   support

      sports     1.0000    1.0000    1.0000      1000
   education     1.0000    0.9980    0.9990      1000
  technology     0.9930    1.0000    0.9965      1000

    accuracy                         0.9985     10000
   macro avg     0.9985    0.9985    0.9985     10000
weighted avg     0.9985    0.9985    0.9985     10000
</pre></li>

<li><p>
&#39044;&#27979;
</p>

<p>
&#20351;&#29992; predict &#26041;&#27861;&#26469;&#39044;&#27979;&#26368;&#21487;&#33021;&#30340;&#31867;&#21035;
</p>
<div>
<pre><span>tokens</span> = [<span>'&#23002;&#26126;'</span>, <span>'&#65306;'</span>, <span>'&#23545;'</span>, <span>'&#22885;&#23612;&#23572;'</span>, <span>'&#19981;&#24471;'</span>, <span>'&#19981;&#26381;'</span>]
model.predict([tokens])          <span># </span><span>=&gt; ['sports']</span>
</pre>
</div>

<p>
&#25110;&#32773;&#29992; predict_top_k_class &#26469;&#33719;&#21462; topk &#30340;&#39044;&#27979;&#32467;&#26524;&#21450;&#20998;&#25968;
</p>
<div>
<pre><span>print</span>(model.predict_top_k_class([tokens], top_k=3))
</pre>
</div>

<p>
&#32467;&#26524;
</p>
<div>
<pre>[
    {
        <span>'label'</span>: <span>'sports'</span>,
        <span>'confidence'</span>: 0.50483656,
        <span>'candidates'</span>: [
            {<span>'label'</span>: <span>'education'</span>, <span>'confidence'</span>: 0.057417843},
            {<span>'label'</span>: <span>'technology'</span>, <span>'confidence'</span>: 0.048766118},
        ]
    }
]
</pre>
</div></li>

<li>&#27169;&#22411;&#20445;&#23384;&#21644;&#35835;&#21462;

<ul>
<li><p>
&#20445;&#23384;
</p>

<p>
&#20351;&#29992; save &#26041;&#27861;&#23558;&#27169;&#22411;&#20445;&#23384;&#21040; test &#30446;&#24405;&#20013;&#65292;&#30446;&#24405;&#19981;&#23384;&#22312;&#20250;&#21019;&#24314;
</p>
<div>
<pre>model.save(<span>'test'</span>)
</pre>
</div>

<p>
&#30446;&#24405;&#20013;&#20250;&#26377;&#19968;&#20010;&#25551;&#36848;&#27169;&#22411;&#32467;&#26500;&#30340; model_info.json &#21644;&#35760;&#24405;&#27169;&#22411;&#21442;&#25968;&#30340; model_weights.h5
</p>
<pre>
test
&#9500;&#9472;&#9472; model_info.json
&#9492;&#9472;&#9472; model_weights.h5

0 directories, 2 files
</pre></li>

<li><p>
&#35835;&#21462;
</p>

<p>
&#20351;&#29992; <code>kashgari.utils.d_model</code> &#26469;&#35835;&#21462;&#27169;&#22411;
</p>
<div>
<pre><span>from</span> kashgari.utils <span>import</span> load_model

<span>model</span> = load_model(<span>'test'</span>)
</pre>
</div></li>
</ul></li>
</ul>
</div>
</div>

<div>
<h4>&#22522;&#20110; BERT &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h4>
<div>
<p>
&#20808;&#19979;&#36733; BERT &#27169;&#22411;&#12290;
</p>

<p>
&#20013;&#25991;&#30340;&#35805;&#21487;&#20197;&#29992; Google &#24320;&#25918;&#30340;: <a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip" rel="noopener noreferrer" target="_blank">https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip</a>
</p>

<p>
&#20013;&#25991;&#27169;&#22411;&#19979;&#36733;&#21518;&#35299;&#21387;&#24471;&#21040; chinese_L-12_H-768_A-12 &#36825;&#20010;&#30446;&#24405;
</p>

<p>
&#28982;&#21518;&#21019;&#24314;&#22522;&#20110; BERT &#30340;&#20998;&#31867;&#22120;
</p>

<div>
<pre><span>from</span> kashgari.embeddings <span>import</span> BERTEmbedding
<span>from</span> kashgari.tasks.classification.models <span>import</span> CNN_Model

<span>embedding</span> = BERTEmbedding(<span>'chinese_L-12_H-768_A-12/'</span>, task=kashgari.CLASSIFICATION)
<span>model</span> = CNN_Model(embedding)
</pre>
</div>

<p>
&#20043;&#21518;&#30340;&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#39044;&#27979;&#65292;&#37117;&#21644;&#38750; BERT &#30340;&#27169;&#22411;&#19968;&#26679;&#12290;
</p>

<p>
&#40664;&#35748;&#24773;&#20917;&#19979; BERTEmbedding &#34987;&#35774;&#32622;&#20026;&#19981;&#21487;&#35757;&#32451;&#65292;&#22914;&#26524;&#38656;&#35201;&#23545; BERT &#36827;&#34892; finetuning &#30340;&#35805;&#65292;&#37027;&#20040;&#25353;&#22914;&#19979;&#35774;&#32622;&#65306;
</p>
<div>
<pre><span>embedding</span> = BERTEmbedding(<span>'chinese_L-12_H-768_A-12/'</span>, task=kashgari.CLASSIFICATION, trainable=<span>True</span>)
</pre>
</div>
</div>
</div>
</div>

<div>
<h3>&#20351;&#29992; AllenNLP &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h3>
<div>
<p>
&#23433;&#35013;: <code>pip install allennlp</code>
</p>

<p>
&#25991;&#26723;: <a href="https://allennlp.org/tutorials" rel="noopener noreferrer" target="_blank">https://allennlp.org/tutorials</a>
</p>
</div>

<div>
<h4>&#36827;&#34892;&#24120;&#35268;&#30340;&#25991;&#26412;&#20998;&#31867;</h4>
<div>
<p>
AllenNLP &#23436;&#20840;&#36890;&#36807;&#37197;&#32622;&#25991;&#20214;&#26469;&#23545;&#25968;&#25454;&#22788;&#29702;&#12289;&#27169;&#22411;&#32467;&#26524;&#21644;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#35774;&#32622;&#65292;&#26368;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#19968;&#34892;&#20195;&#30721;&#19981;&#20889;&#23601;&#25226;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#35757;&#32451;&#20986;&#26469;&#12290;&#19979;&#38754;&#26159;&#19968;&#20010;&#37197;&#32622;&#25991;&#20214;&#31034;&#20363;&#65306;
</p>

<div>
<pre>{
    <span>"dataset_reader"</span>: {
        <span>"type"</span>: <span>"text_classification_json"</span>,
        <span>"tokenizer"</span>: {
            <span>"type"</span>: <span>"word"</span>,
            <span>"word_splitter"</span>: {
                <span>"type"</span>: <span>"jieba"</span>,
            }
        }
    },
    <span>"train_data_path"</span>: <span>"allen.data.train"</span>,
    <span>"test_data_path"</span>: <span>"allen.data.test"</span>,
    <span>"evaluate_on_test"</span>: <span>true</span>,
    <span>"model"</span>: {
        <span>"type"</span>: <span>"basic_classifier"</span>,
        <span>"text_field_embedder"</span>: {
            <span>"tokens"</span>: {
                <span>"type"</span>: <span>"embedding"</span>,
                <span>"embedding_dim"</span>: 100,
                <span>"trainable"</span>: <span>true</span>
            }
        },
        <span>"seq2vec_encoder"</span>: {
            <span>"type"</span>: <span>"cnn"</span>,
            <span>"embedding_dim"</span>: 100,
            <span>"num_filters"</span>: 1,
            <span>"ngram_filter_sizes"</span>: [2, 3, 4]
        }
    },
    <span>"iterator"</span>: {
        <span>"type"</span>: <span>"bucket"</span>,
        <span>"sorting_keys"</span>: [[<span>"tokens"</span>, <span>"num_tokens"</span>]],
        <span>"batch_size"</span>: 64
    },
    <span>"trainer"</span>: {
        <span>"num_epochs"</span>: 40,
        <span>"patience"</span>: 3,
        <span>"cuda_device"</span>: -1,
        <span>"grad_clipping"</span>: 5.0,
        <span>"validation_metric"</span>: <span>"+accuracy"</span>,
        <span>"optimizer"</span>: {
            <span>"type"</span>: <span>"adam"</span>
        }
    }
}
</pre>
</div>

<p>
&#37197;&#32622;&#25991;&#20214;&#20013;&#30340;&#20869;&#23481;&#21487;&#20197;&#20998;&#25104;
</p>
<ul>
<li>&#25968;&#25454;&#37096;&#20998;: &#21253;&#25324; dataset_reader/train_data_path/test_data_path &#36825;&#20960;&#20010; key &#21450;&#20854; value</li>
<li>&#27169;&#22411;&#37096;&#20998;: &#23601;&#26159; model &#36825;&#20010; key &#30340;&#20869;&#23481;</li>
<li>&#35757;&#32451;&#37096;&#20998;: &#21253;&#25324; evaluate_on_test/iterator/trainer &#36825;&#20960;&#20010; key &#21450;&#20854; value</li>
</ul>

<p>
&#30001;&#20110;&#26412;&#25991;&#19981;&#26159;&#19987;&#38376;&#20171;&#32461; AllenNLP &#30340;&#25991;&#31456;&#65292;&#25152;&#20197;&#21482;&#23545;&#36825;&#20123;&#37197;&#32622;&#20570;&#31616;&#35201;&#35828;&#26126;&#65292;&#35814;&#32454;&#20869;&#23481;&#21487;&#26597;&#30475;&#25991;&#26723;&#12290;
</p>
<ul>
<li><p>
&#25968;&#25454;&#37096;&#20998;
</p>

<p>
train_data_path &#21644; test_data_path &#27604;&#36739;&#22909;&#29702;&#35299;&#65292;&#23427;&#20204;&#25351;&#23450;&#20102;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#25991;&#20214;&#36335;&#24452;&#65307;&#32780; data_reader &#21017;&#38480;&#23450;&#20102;&#25968;&#25454;&#25991;&#20214;&#30340;&#26684;&#24335;&#12290;
</p>

<p>
data_reader &#20013;&#30340;&#37197;&#32622;&#65292;&#20250;&#34987;&#29992;&#26469;&#26500;&#24314;&#19968;&#20010; <a href="https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html" rel="noopener noreferrer" target="_blank">DatasetReader</a> &#30340;&#23376;&#31867;&#30340;&#23545;&#35937;&#65292;&#29992;&#26469;&#35835;&#21462;&#25968;&#25454;&#24182;&#36716;&#25442;&#25104;&#19968;&#20010;&#20010; <a href="https://allenai.github.io/allennlp-docs/api/allennlp.data.instance.html" rel="noopener noreferrer" target="_blank">Instance</a> &#23545;&#35937;&#12290;
</p>
<ul>
<li><p>
&#20869;&#32622;&#30340;&#21487;&#29992;&#26469;&#35835;&#21462;&#20998;&#31867;&#25968;&#25454;&#30340; DataReader &#26159; <a href="https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.text_classification_json.html" rel="noopener noreferrer" target="_blank">TextClassificationJsonReader</a> &#65292;&#25152;&#20197;&#37197;&#32622;&#20013;&#26377;
</p>

<pre>
"type": "text_classification_json"
</pre>

<p>
&#36825;&#20010; type &#30340;&#20540;&#26159; TextClassificationJsonReader &#36825;&#20010;&#31867;&#23454;&#29616;&#30340;&#26102;&#20505;&#27880;&#20876;&#19978;&#30340;&#65292;&#21435;&#30475;&#20195;&#30721;&#20250;&#30475;&#21040;&#26377;&#36825;&#26679;&#30340;&#29255;&#27573;
</p>
<div>
<pre><span>@DatasetReader.register</span>(<span>"text_classification_json"</span>)
<span>class</span> <span>TextClassificationJsonReader</span>(DatasetReader):
</pre>
</div>

<p>
&#36825;&#20010; TextClassificationJsonReader &#35201;&#27714;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#19968;&#34892;&#19968;&#20010; json &#25968;&#25454;&#65292;&#22914;&#19979;&#65306;
</p>
<pre>
{"label": "education", "text": "&#21517;&#24072;&#25351;&#23548;&#25176;&#31119;&#35821;&#27861;&#25216;&#24039;&#65306;&#21517;&#35789;&#30340;&#22797;&#25968;&#24418;&#24335;"}
{"label": "education", "text": "&#20013;&#22269;&#39640;&#32771;&#25104;&#32489;&#28023;&#22806;&#35748;&#21487;&#26159;&ldquo;&#29436;&#26469;&#20102;&rdquo;&#21527;&#65311;"}
{"label": "sports, "text": "&#22270;&#25991;&#65306;&#27861;&#32593;&#23391;&#33778;&#23572;&#26031;&#33510;&#25112;&#36827;16&#24378;&#23391;&#33778;&#23572;&#26031;&#24594;&#21564;"}
{"label": "sports, "text": "&#22235;&#24029;&#20025;&#26865;&#20030;&#34892;&#20840;&#22269;&#38271;&#36317;&#30331;&#23665;&#25361;&#25112;&#36187;&#36817;&#19975;&#20154;&#21442;&#19982;"}
</pre></li>

<li><p>
DataReader &#36890;&#36807;&#37197;&#32622;&#20013; tokenizer &#37096;&#20998;&#20250;&#21019;&#24314;&#19968;&#20010;&#20998;&#35789;&#22120;&#65292;&#29992;&#26469;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#35789;&#24207;&#21015;
</p>

<pre>
"tokenizer": {
    "type": "word",
    "word_splitter": {
        "type": "jieba",
    }
}
</pre>

<p>
type &#30340;&#20540;&#35774;&#32622;&#20026; word&#65292;&#36825;&#27809;&#20160;&#20040;&#22909;&#35828;&#30340;&#12290;
</p>

<p>
tokenizer &#20013;&#30340; word_splitter &#25351;&#23450;&#30340;&#25165;&#26159;&#30495;&#27491;&#30340;&#20998;&#35789;&#22120;&#65288;&#27604;&#36739;&#32469;&#65289;&#12290;
</p>

<p>
&#22914;&#26524;&#26159;&#33521;&#25991;&#30340;&#25968;&#25454;&#65292;&#37027;&#20040; word_splitter &#30340;&#37197;&#32622;&#21487;&#20197;&#19981;&#20889;&#65292;&#40664;&#35748;&#23601;&#26159;&#25903;&#25345;&#33521;&#25991;&#20998;&#35789;&#30340;&#12290;
</p>

<p>
&#20294;&#22914;&#26524;&#26159;&#29992;&#20110;&#20013;&#25991;&#22788;&#29702;&#30340;&#35805;&#65292;&#26377;&#19968;&#20010; SpacyWordSplitter &#21487;&#20197;&#29992;&#20110;&#20013;&#25991;&#20998;&#31867;&#65292;&#20294;&#26159;&#29616;&#26377;&#30340;<a href="https://github.com/howl-anderson/Chinese_models_for_SpaCy" rel="noopener noreferrer" target="_blank">&#20013;&#25991; spaCy &#27169;&#22411;</a>&#20165;&#25903;&#25345; spaCy 2.0.x&#65292;&#21644; AllenNLP &#20013; spaCy &#35201;&#27714;&#30340;&#29256;&#26412;&#19981;&#20860;&#23481;&#65292;&#36825;&#20010;&#26159;&#27604;&#36739;&#22353;&#30340;&#12290;
</p>

<p>
&#22909;&#22312; AllenNLP &#25552;&#20379;&#20102;&#21152;&#36733;&#33258;&#23450;&#20041;&#27169;&#22359;&#30340;&#26041;&#27861;&#65292;&#25353;&#29031;&#22914;&#19979;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;
</p>
<div>
<pre>mkdir allen_ext/
touch allen_ext/__init__.py
touch allen_ext/word_splitter.py
</pre>
</div>

<p>
&#28982;&#21518;&#22312; allen_ext/word_splitter.py &#20013;&#20889;&#20837;&#22914;&#19979;&#20869;&#23481;
</p>
<div>
<pre><span>from</span> typing <span>import</span> List

<span>import</span> jieba
<span>from</span> overrides <span>import</span> overrides
<span>from</span> allennlp.data.tokenizers.token <span>import</span> Token
<span>from</span> allennlp.data.tokenizers.word_splitter <span>import</span> WordSplitter


<span>@WordSplitter.register</span>(<span>'jieba'</span>)
<span>class</span> <span>JiebaWordSplitter</span>(WordSplitter):

    <span>def</span> <span>__init__</span>(<span>self</span>):
        <span>pass</span>

    <span>@overrides</span>
    <span>def</span> <span>split_words</span>(<span>self</span>, sentence: <span>str</span>) -&gt; List[Token]:
        <span>offset</span> = 0
        <span>tokens</span> = []
        <span>for</span> word <span>in</span> jieba.lcut(sentence):
            <span>word</span> = word.strip()
            <span>if</span> <span>not</span> word:
                <span>continue</span>

            <span>start</span> = sentence.find(word, offset)
            tokens.append(Token(word, start))

            <span>offset</span> = start + <span>len</span>(word)

        <span>return</span> tokens
</pre>
</div>

<p>
&#20351;&#29992; <code>WordSplitter.register('jieba')</code> &#21518;&#23601;&#21487;&#20197;&#22312;&#37197;&#32622;&#20013; word_splitter &#37096;&#20998;&#20889;&#19978; <code>"type": "jieba"</code> &#26469;&#21551;&#29992;&#12290;
</p>

<p>
&#22312; allen_ext/__init__.py &#20013;&#20889;&#20837;&#22914;&#19979;&#20869;&#23481;
</p>
<div>
<pre><span>from</span> .word_splitter <span>import</span> JiebaWordSplitter

<span>__all__</span> = [<span>'JiebaWordSplitter'</span>]
</pre>
</div>

<p>
&#33258;&#23450;&#20041;&#20102; JiebaWordSplitter &#21518;&#22312;&#35757;&#32451;&#30340;&#26102;&#20505;&#36824;&#35201;&#21152;&#36733; allen_ext &#36825;&#20010;&#30446;&#24405;&#25165;&#33021;&#29983;&#25928;&#65292;&#36825;&#20010;&#20043;&#21518;&#20877;&#35828;&#12290;
</p></li>
</ul></li>

<li><p>
&#27169;&#22411;&#37096;&#20998;
</p>

<p>
&#22240;&#20026;&#26159;&#20570;&#25991;&#26412;&#20998;&#31867;&#65292;&#25152;&#20197; type &#35774;&#32622;&#20026; <a href="https://allenai.github.io/allennlp-docs/api/allennlp.models.basic_classifier.html" rel="noopener noreferrer" target="_blank">basic_classifier</a>&#12290;
</p>

<p>
&#36825;&#20010;&#20998;&#31867;&#22120;&#38656;&#35201; text_field_embedder &#21644; seq2vec_encoder &#20004;&#20010;&#21442;&#25968;&#65306;
</p>
<ul>
<li><p>
text_field_embedder &#29992;&#26469;&#23450;&#20041; word embedding&#65292;&#36825;&#20010;&#37197;&#32622;&#24212;&#35813;&#36824;&#22909;&#29702;&#35299;
</p>

<pre>
"text_field_embedder": {
    "tokens": {
        "type": "embedding",
        "embedding_dim": 100,
        "trainable": true
    }
}
</pre></li>

<li><p>
seq2vec_encoder &#21017;&#29992;&#26469;&#20135;&#29983;&#21477;&#23376;&#30340;&#32534;&#30721;&#21521;&#37327;&#29992;&#20110;&#20998;&#31867;&#65292;&#36825;&#37324;&#36873;&#25321;&#20102; CNN
</p>

<pre>
"seq2vec_encoder": {
    "type": "cnn",
    "embedding_dim": 100,
    "num_filters": 1,
    "ngram_filter_sizes": [2, 3, 4]
}
</pre></li>
</ul></li>

<li>&#35757;&#32451;&#37096;&#20998;&#65306;&#30053;</li>
</ul>


<p>
&#37197;&#32622;&#25991;&#20214;&#20889;&#22909;&#21518;&#65292;&#20551;&#35774;&#37197;&#32622;&#25991;&#20214;&#20026; config.json&#65292;&#30452;&#25509;&#25191;&#34892;&#19979;&#38754;&#30340;&#21629;&#20196;&#26469;&#35757;&#32451;&#21363;&#21487;
</p>
<div>
<pre>allennlp train config.json -s model_save_dir --include-package allen_ext
</pre>
</div>

<p>
&#36873;&#39033; <code>--include-package allen_ext</code> &#29992;&#26469;&#26469;&#21152;&#36733;&#33258;&#23450;&#20041;&#30340;&#27169;&#22359;&#12290;
</p>

<p>
&#26368;&#32456;&#20250;&#22312; save_dir &#30446;&#24405;&#19979;&#20135;&#29983;&#19968;&#20010; model.tar.gz &#25991;&#20214;&#65292;&#23601;&#26159;&#27169;&#22411;&#21442;&#25968;&#65292;&#28982;&#21518;&#30446;&#24405;&#19979;&#36824;&#20250;&#20135;&#29983; tensorboard &#33021;&#35835;&#21462;&#30340; log&#65292;&#36825;&#20010;&#25402;&#26041;&#20415;&#30340;&#12290;
</p>

<p>
&#35780;&#20272;&#30340;&#35805;&#65292;&#29992; evaluate &#21629;&#20196;
</p>
<pre>
allennlp evaluate model_save_dir/model.tar.gz test.jsonl --include-package allen_ext
</pre>

<p>
&#27604;&#36739;&#40635;&#28902;&#30340;&#26159;&#65292;&#39044;&#27979;&#38656;&#35201;&#19968;&#20010; Predictor&#65292;&#32780; AllenNLP &#20013;&#20869;&#32622;&#30340; <a href="https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html#text-classifier" rel="noopener noreferrer" target="_blank">TextClassifierPredictor</a> &#35201;&#27714;&#30340;&#36755;&#20837;&#26159; <code>{"sentence": "xxx"}</code> &#65292;&#36825;&#20010;&#21644; TextClassificationJsonReader &#30340;&#35201;&#27714;&#19981;&#19968;&#26679;&hellip;&hellip;
</p>

<p>
&#22914;&#26524;&#26159;&#22312;&#20195;&#30721;&#37324;&#36827;&#34892;&#39044;&#27979;&#65292;&#37027;&#20040;&#26159;&#27809;&#26377;&#38382;&#39064;&#30340;&#65292;&#21487;&#20197;&#36825;&#26679;
</p>
<div>
<pre><span>from</span> allen_ext <span>import</span> *         <span># </span><span>noqa</span>
<span>from</span> allennlp.models.archival <span>import</span> load_archive
<span>from</span> allennlp.predictors.predictor <span>import</span> Predictor

<span>archive</span> = load_archive(<span>'model_save_dir/model.tar.gz'</span>)
<span>predictor</span> = Predictor.from_archive(archive)

<span>inputs</span> = {<span>"sentence"</span>: <span>"&#21517;&#24072;&#25351;&#23548;&#25176;&#31119;&#35821;&#27861;&#25216;&#24039;&#65306;&#21517;&#35789;&#30340;&#22797;&#25968;&#24418;&#24335;"</span>}
<span>result</span> = predictor.predict_json(inputs)
</pre>
</div>

<p>
&#24471;&#21040;&#30340; result &#26159;&#36825;&#26679;&#30340;&#32467;&#26500;
</p>
<div>
<pre>{
    <span>'label'</span>: <span>'education'</span>,
    <span>'logits'</span>: [
        15.88630199432373,
        0.7209644317626953,
        7.292031764984131,
        5.195938587188721,
        5.073373317718506,
        -35.6490478515625,
        -7.7982988357543945,
        -35.44648742675781,
        -18.14293098449707,
        -14.513381004333496
    ],
    <span>'probs'</span>: [
        0.999771773815155,
        2.592259420453047e-07,
        0.0001851213601185009,
        2.2758060367777944e-05,
        2.013285666180309e-05,
        4.153195524896307e-23,
        5.1737975015342386e-11,
        5.085729773519049e-23,
        1.6641527142180782e-15,
        6.273159211056881e-14
    ],
}
</pre>
</div>

<p>
&#36825;&#20010;&#36755;&#20986;&#32467;&#26500;&#23436;&#20840;&#26159;&#30001; TextClassifierPredictor &#20915;&#23450;&#30340;&#12290;
</p>

<p>
&#22914;&#26524;&#35201;&#33258;&#23450;&#20041; Predictor&#65292;&#21487;&#20197;&#21442;&#32771;<a href="https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/predicting_paper_venues/predicting_paper_venues_pt2.md#creating-a-predictor" rel="noopener noreferrer" target="_blank">&#25991;&#26723;</a>&#12290;
</p>
</div>
</div>

<div>
<h4>&#22522;&#20110; BERT &#36827;&#34892;&#25991;&#26412;&#20998;&#31867;</h4>
<div>
<p>
AllenNLP &#26159;&#22522;&#20110; pytorch &#23454;&#29616;&#30340;&#65292;&#25152;&#20197; Google &#25552;&#20379;&#30340; BERT &#27169;&#22411;&#22312;&#23427;&#36825;&#37324;&#27809;&#27861;&#29992;&#65292;&#38656;&#35201;&#19979;&#36733;&#23427;&#33258;&#24049;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#20197;&#20013;&#25991;&#27169;&#22411;&#20026;&#20363;&#65306;
</p>
<div>
<pre>mkdir chinese_bert_torch &amp;&amp; <span>cd</span> chinese_bert_torch
wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin -O pytorch_model.bin
wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json -O config.json
wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt -O vocab.txt
</pre>
</div>

<p>
&#28982;&#21518; config.json &#20013; data_reader &#37096;&#20998;&#36825;&#26679;&#20889;
</p>
<div>
<pre>{
    <span>"dataset_reader"</span>: {
        <span>"type"</span>: <span>"text_classification_json"</span>,
        <span>"tokenizer"</span>: {
            <span>"type"</span>: <span>"word"</span>,
            <span>"word_splitter"</span>: {
                <span>"type"</span>: <span>"bert-basic"</span>,
            }
        },
        <span>"token_indexers"</span>: {
            <span>"bert"</span>: {
                <span>"type"</span>: <span>"bert-pretrained"</span>,
                <span>"pretrained_model"</span>: <span>"./chinese_bert_torch/vocab.txt"</span>
            }
        }
    }
}
</pre>
</div>

<p>
model &#37096;&#20998;&#36825;&#20040;&#20889;
</p>
<div>
<pre>{
    <span>"model"</span>: {
        <span>"type"</span>: <span>"bert_for_classification"</span>,
        <span>"bert_model"</span>: <span>"./chinese_bert_torch"</span>,
        <span>"trainable"</span>: <span>false</span>
    }
}
</pre>
</div>

<p>
&#36825;&#37324; trainable &#35774;&#32622;&#25104; false &#30340;&#35805; BERT &#23601;&#21482;&#26159;&#20805;&#24403;&#19968;&#20010; encoder&#65292;&#19981;&#21442;&#19982;&#35757;&#32451;&#65307;&#22914;&#26524;&#35201;&#36827;&#34892; finetuning &#30340;&#35805;&#23558;&#20854;&#25913;&#20026; true&#12290;
</p>

<p>
&#23436;&#25972;&#30340;&#37197;&#32622;&#26159;&#36825;&#20010;&#26679;&#23376;&#30340;
</p>
<div>
<pre>{
    <span>"dataset_reader"</span>: {
        <span>"type"</span>: <span>"text_classification_json"</span>,
        <span>"tokenizer"</span>: {
            <span>"type"</span>: <span>"word"</span>,
            <span>"word_splitter"</span>: {
                <span>"type"</span>: <span>"bert-basic"</span>,
            }
        },
        <span>"token_indexers"</span>: {
            <span>"bert"</span>: {
                <span>"type"</span>: <span>"bert-pretrained"</span>,
                <span>"pretrained_model"</span>: <span>"./chinese_bert_torch/vocab.txt"</span>
            }
        }
    },
    <span>"train_data_path"</span>: <span>"allen.data.train"</span>,
    <span>"test_data_path"</span>: <span>"allen.data.test"</span>,
    <span>"evaluate_on_test"</span>: <span>true</span>,
    <span>"model"</span>: {
        <span>"type"</span>: <span>"bert_for_classification"</span>,
        <span>"bert_model"</span>: <span>"./chinese_bert_torch"</span>,
        <span>"trainable"</span>: <span>false</span>
    },
    <span>"iterator"</span>: {
        <span>"type"</span>: <span>"bucket"</span>,
        <span>"sorting_keys"</span>: [[<span>"tokens"</span>, <span>"num_tokens"</span>]],
        <span>"batch_size"</span>: 64
    },
    <span>"trainer"</span>: {
        <span>"num_epochs"</span>: 5,
        <span>"patience"</span>: 3,
        <span>"cuda_device"</span>: -1,
        <span>"grad_clipping"</span>: 5.0,
        <span>"validation_metric"</span>: <span>"+accuracy"</span>,
        <span>"optimizer"</span>: {
            <span>"type"</span>: <span>"adam"</span>
        }
    }
}
</pre>
</div>

<p>
&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#39044;&#27979;&#31561;&#25805;&#20316;&#21516;&#26410;&#20351;&#29992; BERT &#30340;&#26102;&#20505;&#19968;&#26679;&#12290;
</p>
</div>
</div>
</div>
</div>